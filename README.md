ASL Interpreting Glove

Team Members

Kris Gurung

Anirudhhan Raghuraman

Madalyn Wiley

Project Goals

The objective of this project is to design an American Sign Language (ASL)-interpreting glove that translates sign language into English and outputs the translation through audio and a visual user interface.

Features:

Recognition of the entire ASL alphabet

Interpretation of basic static signs

Future Enhancements:

Dynamic sign recognition

Integration of two gloves for improved accuracy

Development of a PCB for a more compact design

Necessary Equipment and Sensors

(2) ESP32 microcontrollers

(15) Flex Sensors

(2) 8 Ohm Speakers

(2) MPU6050 Motion Sensors

(1) Pair of Gloves

Project Description

The ASL-interpreting glove will use a combination of flex sensors and motion sensors to detect hand movements and gestures. The detected gestures will be processed using an ESP32 microcontroller, and the translated text will be output via a speaker and a visual user interface.

Learning Goals

Understanding how to store and process sensor data

Measuring sensor values using an ESP32 microcontroller

Integrating an ESP32 with an Android application

Combining multiple components into a functional final product

Comparison to Existing Models

Several models exist for ASL interpretation, including:

UCLAâ€™s Sign-to-Speech Translating Glove

Other sensor-based ASL gloves with different sensor configurations and processing techniques
